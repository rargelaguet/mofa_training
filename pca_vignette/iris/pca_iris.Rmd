---
title: "Principal Component Analysis"
author: 
  name: "Ricard Argelaguet"
  affiliation: "European Bioinformatics Institute, Cambridge, UK"
  email: "ricard@ebi.ac.uk"
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc: true
    toc_depth: 2
---

# Load libraries

```{r, message = FALSE}
library(ggplot2)
library(tidyverse)
library(ggpubr)
```

# Load the data

the `iris` data set is one of the most popular for the study of machine learning techniques. Read more about it [here](https://www.kaggle.com/uciml/iris)
```{r}
data(iris)
head(iris)
```

```{r}
Y <- iris[-5] %>% as.matrix %>% scale(center=TRUE, scale=FALSE)
N <- nrow(Y)
D <- ncol(Y)
```

# Compute PCA using the eigen decomposition of the covariance matrix

Calculate covariance matrix
```{r}
S <- (t(Y)%*%Y) / (N-1)
```

Calculate the rank 2 [eigendecomposition](https://rpubs.com/aaronsc32/eigenvalues-eigenvectors-r) of the covariance matrix. In some resources they call this the rotation matrix. It corresponds to the feature weights of the matrix factorisation scheme
```{r}
eigenvectors <- eigen(S)$vectors[,c(1,2)]
head(eigenvectors) 
```

Eigenvalues correspond to the standard deviation of the principal components
```{r}
eigenvalues <- eigen(S)$values[c(1,2)]
head(sqrt(eigenvalues))
```

Multipling the original data by the eigenvectors projects the high-dimensional data onto the low-dimensional space (our principal components)
```{r}
PCs <- Y %*% eigenvectors
head(PCs)
```

Each Principal Component (PC) corresponds to an orthogonal axis of variation. Each PC is made up by a linear combination of the input features, where the linear coeficients are given by the feature weights. Each PC ordinates the samples along a one-dimensional axis centered at zero:
```{r}
df <- as.data.frame(PCs) %>% 
  cbind(as.character(iris$Species)) %>%
  `colnames<-`(c("PC1","PC2","species"))

ggscatter(df, x="PC1", y="PC2", fill="species", shape=21 ,size=3) +
  labs(x="Principal component 1", y="Principal component 2") +
  theme(
    legend.position = "right",
    axis.text = element_text(size=rel(0.8))
  )
```

# Compare our manual PCA to the standard `prcomp` function

The standard way to perform Principal Component Analysis in R is to use the `prcomp` function. The underlying algorithm is slightly different (i.e. more numerically stable) than the eigenvector decomposition.

```{r}
prcomp.pca <- prcomp(Y, rank.=2)
names(prcomp.pca)
```

`rotation` corresponds to the "weight matrix" (the matrix whose columns contain the eigenvectors)
```{r}
dim(prcomp.pca$rotation)
head(prcomp.pca$rotation)
```

`x` corresponds to the principal components (the data multiplied by the rotation matrix)
```{r}
dim(prcomp.pca$x)
head(prcomp.pca$x)
```


`sdev` contains the standard deviation explained by each PC
```{r}
prcomp.pca$sdev[1:2]
```

Rotate PCs for visualisation (ignore this)
```{r echo=FALSE}
prcomp.pca$x[,2] <- -(prcomp.pca$x[,2])
prcomp.pca$rotation[,2] <- -(prcomp.pca$rotation[,2])
```

Plot the PCs
```{r}
df <- as.data.frame(prcomp.pca$x) %>% 
  cbind(iris$Species) %>%
  `colnames<-`(c("PC1","PC2","species"))

ggscatter(df, x="PC1", y="PC2", fill="species", shape=21 ,size=3) +
  labs(x="Principal component 1", y="Principal component 2") +
  theme(
    legend.position = "right",
    axis.text = element_text(size=rel(0.8))
  )
```


# Exploration of the feature weights

The feature weights (also called loadings) provide a score for how strong each feature relates to each factor. Features with no linear association with the PC have values close to zero whereas features with strong association with the factor have large absolute values. The sign of the weight indicates the direction of the effect: a positive weight indicates that the feature has higher levels in the samples with positive factor values, and vice versa.

```{r}
to.plot <- prcomp.pca$rotation %>%
  as.data.frame %>% tibble::rownames_to_column("feature") %>%
  pivot_longer(cols=starts_with("PC"), names_to = "PC", values_to = "value")

ggbarplot(to.plot, x="feature", y="value", facet="PC", fill="feature", position = position_dodge(0.9)) +
  labs(x="", y="Loading") +
  theme(
    legend.position = "right",
    legend.title = element_blank(),
    axis.text.x = element_blank()
  )
```

Petal.Length has a positive loading for PC1. This means that the Petal Length increases with increasing PC1 values (from negative to positive). In contrast, it has a loading of almost zero for PC2, which suggests that it has no influence on this source of variation. Let's check this:
```{r}
to.plot <- as.data.frame(prcomp.pca$x) %>%
  mutate(petal_length=iris$Petal.Length) %>%
  pivot_longer(cols=starts_with("PC"), names_to = "PC", values_to = "pc_value")
  
ggscatter(to.plot, x="pc_value", y="petal_length", facet="PC", scales="free",
  add="reg.line", add.params = list(color="blue", fill="lightgray"), conf.int=TRUE) +
  stat_cor(method = "pearson") +
  labs(x="PC value", y="Petal length") +
  theme(
    axis.text = element_text(size=rel(0.8))
  )
```

# Questions

- **(Q) Why are the feature means not important when doing PCA (i.e. why is the data centered)?**  
- **(Q) Does the PCA solution change if you scale the features with `scale(center=TRUE, scale=TRUE)`? If so, why? When would you apply scaling to your data?**
